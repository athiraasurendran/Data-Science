# Deep Learning

- Deep Learning (DL) is a **subset of Machine Learning** that uses **neural networks** with multiple layers to learn from data.  
- Can automatically extract **features** from raw data without manual feature engineering.  
- Commonly used for **image recognition, NLP, speech recognition, and autonomous systems**.

---

## Difference: Machine Learning vs Deep Learning

| Feature | Machine Learning | Deep Learning |
|---------|-----------------|---------------|
| Feature Extraction | Manual | Automatic |
| Data Requirement | Small to medium | Large datasets |
| Hardware | CPU | GPU (preferred) |
| Training Time | Short | Long |
| Complexity | Less complex | More complex |

---

## Neural Networks Basics

- **Neurons** → Basic units that process information.  
- **Layers** →  
  - Input Layer → Accepts data  
  - Hidden Layers → Process data using **activation functions**  
  - Output Layer → Produces prediction  

### Activation Functions

- **Sigmoid** → Outputs between 0 and 1  
- **ReLU (Rectified Linear Unit)** → Outputs 0 if negative, else linear  
- **Tanh** → Outputs between -1 and 1  
- **Softmax** → Converts outputs into probabilities for multi-class classification

---

## Deep Learning Architecture

1. **Feedforward Neural Network (FNN)** → Data flows **forward** only.  
2. **Convolutional Neural Network (CNN)** → Used for **image/video tasks**, extracts spatial features.  
3. **Recurrent Neural Network (RNN)** → Handles **sequential data** (time series, text).  
   - Variants: LSTM, GRU  
4. **Autoencoders** → Unsupervised DL for **dimensionality reduction** and **anomaly detection**.  
5. **Generative Models** → Generate new data (GANs, Variational Autoencoders).

---

## Deep Learning Workflow

1. **Data Collection** → Images, text, audio, or sensor data.  
2. **Data Preprocessing** → Normalization, tokenization, resizing images.  
3. **Model Design** → Choose network architecture and layers.  
4. **Training** → Feed data, optimize weights using **backpropagation**.  
5. **Evaluation** → Metrics: Accuracy, Precision, Recall, F1-score, RMSE.  
6. **Deployment** → Serve model with Flask, FastAPI, or Streamlit.  

---

## Common Deep Learning Libraries

| Library | Purpose |
|---------|---------|
| TensorFlow | Build and train neural networks |
| Keras | High-level API for TensorFlow |
| PyTorch | Deep learning research & production |
| OpenCV | Computer vision preprocessing |
| Hugging Face Transformers | NLP models and pipelines |

---

## Summary
- Deep Learning enables machines to **learn complex patterns** from large datasets.  
- Works best with **large datasets and high computational power (GPUs)**.  
- Used in **computer vision, natural language processing, speech recognition, and AI research**.

---


# Deep Learning Models

## 1. Feedforward Neural Network (FNN)

- **Also called:** Multilayer Perceptron (MLP)  
- **Description:** Basic neural network where data flows **only forward** through layers.  
- **Use Case:** Simple regression or classification tasks.

### Example (Classification)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(input_dim,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # binary classification

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)
```

## 2. Convolutional Neural Network (CNN)

- Description: Designed for image and spatial data. Uses convolutional layers to extract features.

- Use Case: Image classification, object detection, face recognition.

### Example (Image Classification)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential()
model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D((2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))  # 10 classes

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)
```

## 3. Recurrent Neural Network (RNN)

- Description: Processes sequential data (time series, text) using memory from previous steps.

- Use Case: Text generation, sentiment analysis, stock price prediction.

### Example (Simple RNN for Sequence Data)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

model = Sequential()
model.add(SimpleRNN(50, activation='tanh', input_shape=(timesteps, features)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=10, batch_size=32)
```

## 4. Long Short-Term Memory (LSTM)

- Description: Variant of RNN that solves the vanishing gradient problem. Can capture long-term dependencies.

- Use Case: Time series forecasting, language modeling, speech recognition.

### Example (LSTM for Time Series)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(timesteps, features)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=20, batch_size=32)
```
## 5. Autoencoder

- Description: Unsupervised network used for dimensionality reduction and feature learning.

- Use Case: Anomaly detection, denoising images.

### Example

```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

input_dim = X_train.shape[1]
input_layer = Input(shape=(input_dim,))
encoded = Dense(32, activation='relu')(input_layer)
decoded = Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.fit(X_train, X_train, epochs=50, batch_size=32)
```

6. Generative Adversarial Network (GAN)

- Description: Consists of Generator (creates fake data) and Discriminator (distinguishes real vs fake).

- Use Case: Image generation, data augmentation, deepfakes.

### GAN Concept

```rust
Random Noise --> Generator --> Fake Data --> Discriminator --> Real/Fake Prediction
```
