# Large Language Model (LLM)

- **LLM (Large Language Model)** is a type of **AI model** trained on massive text datasets to understand, generate, and manipulate human language.  
- They are based on **deep learning**, especially **transformer architectures**.  
- Examples: GPT (OpenAI), BERT (Google), LLaMA (Meta), Claude (Anthropic).

---

## Key Features

- **Text Understanding:** Can read and comprehend text.  
- **Text Generation:** Can write coherent text, summaries, or code.  
- **Few-shot / Zero-shot Learning:** Performs tasks with little or no task-specific training.  
- **Context Awareness:** Can generate contextually relevant responses.  

---

## Transformer Architecture (Core of LLMs)

- Introduced in **“Attention is All You Need” (2017)**.  
- Key components:  
  1. **Encoder** → Reads input text (used in models like BERT).  
  2. **Decoder** → Generates output text (used in models like GPT).  
  3. **Self-Attention** → Understands relationships between words in a sentence.  

### Transformer Flow

Input Text → Tokenization → Embeddings → Attention Layers → Feedforward Layers → Output Tokens

---

## Common LLMs

| Model | Developer | Type | Use Case |
|-------|-----------|------|---------|
| GPT-4 | OpenAI | Decoder-only | Text generation, chatbots, code |
| GPT-3 | OpenAI | Decoder-only | Text completion, summarization |
| BERT | Google | Encoder-only | Text classification, QA |
| RoBERTa | Facebook | Encoder-only | NLP tasks, classification |
| LLaMA | Meta | Decoder-only | Research & language tasks |
| Claude | Anthropic | Decoder-only | Chatbots, reasoning |

---
