# GPT (Generative Pre-trained Transformer)

- **GPT** is a **Large Language Model (LLM)** developed by OpenAI.  
- Based on the **transformer architecture**, GPT is designed for **natural language understanding and generation**.  
- Pre-trained on **massive text datasets** and fine-tuned for specific tasks.  

---

## Key Features

- **Text Generation:** Can generate human-like text.  
- **Few-shot / Zero-shot Learning:** Can perform tasks with few or no examples.  
- **Context Awareness:** Generates contextually relevant responses.  
- **Multi-task Capability:** Summarization, translation, coding, Q&A, and more.

---

## GPT Architecture

- Based on **Decoder-only Transformers**.  
- Uses **self-attention** to understand relationships between tokens.  
- Key components:  
  1. **Embedding Layer** → Converts tokens to vectors.  
  2. **Positional Encoding** → Adds order information.  
  3. **Transformer Decoder Blocks** → Multi-head attention + feed-forward layers.  
  4. **Output Layer** → Predicts next token probabilities.
 
---

### GPT Flow

Input Text → Tokenization → Embeddings → Transformer Decoder Blocks → Output Tokens

---

## GPT Versions

| Version | Key Features | Use Cases |
|---------|-------------|-----------|
| GPT-1   | 117M parameters | Basic text generation |
| GPT-2   | 1.5B parameters | High-quality text generation, creative writing |
| GPT-3   | 175B parameters | Advanced NLP tasks: translation, coding, Q&A |
| GPT-4   | Multi-modal, improved reasoning | Chatbots, reasoning, code generation, text summarization |

---

## How GPT Works

1. **Pre-training** → Learns from large text corpus to predict next word.  
2. **Fine-tuning** → Adjusted for specific tasks with smaller datasets.  
3. **Prompting** → Users provide instructions to guide the output.  
4. **Generation** → Uses probability distribution to predict the next token.

---

## Applications of GPT

- **Chatbots & Virtual Assistants:** ChatGPT  
- **Text Summarization:** Summarize articles or documents  
- **Question Answering:** Customer support, educational tools  
- **Code Generation:** GitHub Copilot  
- **Creative Writing:** Stories, poems, scripts  
- **Translation:** Language conversion  
- **Data Analysis:** Generate SQL queries or insights  

---

## Example: Using GPT via OpenAI API

```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")

prompt = "Write a short poem about data science."

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```
---

## GPT Best Practices

- Be clear in prompts → Specific instructions yield better results.

- Provide context → Include relevant background in the prompt.

- Use structured formats → JSON or tables for consistent output.

- Experiment with temperature & max tokens → Control randomness and length of output.
