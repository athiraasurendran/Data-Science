# GPT (Generative Pre-trained Transformer)

- **GPT** is a **Large Language Model (LLM)** developed by OpenAI.  
- Based on the **transformer architecture**, GPT is designed for **natural language understanding and generation**.  
- Pre-trained on **massive text datasets** and fine-tuned for specific tasks.  

---

## Key Features

- **Text Generation:** Can generate human-like text.  
- **Few-shot / Zero-shot Learning:** Can perform tasks with few or no examples.  
- **Context Awareness:** Generates contextually relevant responses.  
- **Multi-task Capability:** Summarization, translation, coding, Q&A, and more.

---

## GPT Architecture

- Based on **Decoder-only Transformers**.  
- Uses **self-attention** to understand relationships between tokens.  
- Key components:  
  1. **Embedding Layer** → Converts tokens to vectors.  
  2. **Positional Encoding** → Adds order information.  
  3. **Transformer Decoder Blocks** → Multi-head attention + feed-forward layers.  
  4. **Output Layer** → Predicts next token probabilities.
 
---

### GPT Flow

Input Text → Tokenization → Embeddings → Transformer Decoder Blocks → Output Tokens

---
