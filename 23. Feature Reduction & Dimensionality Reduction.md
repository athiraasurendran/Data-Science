# Feature Reduction & Dimensionality Reduction  

---

## 1. Feature Reduction  
- Process of removing **irrelevant or less important features** from the dataset.  
- Helps to:  
  - Improve model performance.  
  - Reduce overfitting.  
  - Decrease training time.  
- Methods:  
  - **Manual Selection** → Drop features based on domain knowledge.  
  - **Statistical Tests** → (Chi-square, ANOVA).  
  - **Model-based selection** → Use feature importance from models (Decision Trees, Random Forest).  

---

## 2. Dimensionality Reduction  
- Process of converting data from **high-dimensional space** into **low-dimensional space** while keeping important information.  
- Goal:  
  - Remove noise.  
  - Improve visualization.  
  - Speed up training.  
- Techniques:  
  - **PCA (Principal Component Analysis)**.  
  - **LDA (Linear Discriminant Analysis)**.  
  - **t-SNE / UMAP** (for visualization).  

---

## 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  
- **Clustering algorithm** that groups together points in dense regions.  
- Points in low-density regions are marked as **outliers (noise)**.  
- Parameters:  
  - `eps` → maximum distance between two points to be considered neighbors.  
  - `min_samples` → minimum number of points to form a dense region.  
- Advantages:  
  - No need to specify the number of clusters.  
  - Handles irregular cluster shapes and noise.  

### DBSCAN Code Example  
```python
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.3, min_samples=5)
db.fit(X)
labels = db.labels_
