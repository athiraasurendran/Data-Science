# Ensemble Learning & Bias-Variance Trade-Off  

---

## 1. Bias-Variance Trade-Off  
- **Bias** → Error from making the model too simple (underfitting).  
- **Variance** → Error from making the model too complex (overfitting).  
- Goal: Find a balance between **low bias** and **low variance** for the best generalization.  

---

## 2. Bagging (Bootstrap Aggregation)  
- **B**ootstrapped **Agg**regation.  
- Works by training multiple models on **random subsets** of the data (with replacement).  
- Final result = **average** (for regression) or **majority vote** (for classification).  
- Uses **high variance, low bias models** as base learners.  
- Example: **Random Forest**.  
- Advantage: **Reduces variance**, prevents overfitting.  
- Easy to run in **parallel**.  

---
