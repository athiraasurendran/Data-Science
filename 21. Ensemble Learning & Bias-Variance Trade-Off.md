# Ensemble Learning & Bias-Variance Trade-Off  

---

## 1. Bias-Variance Trade-Off  
- **Bias** → Error from making the model too simple (underfitting).  
- **Variance** → Error from making the model too complex (overfitting).  
- Goal: Find a balance between **low bias** and **low variance** for the best generalization.  

---

## 2. Bagging (Bootstrap Aggregation)  
- **B**ootstrapped **Agg**regation.  
- Works by training multiple models on **random subsets** of the data (with replacement).  
- Final result = **average** (for regression) or **majority vote** (for classification).  
- Uses **high variance, low bias models** as base learners.  
- Example: **Random Forest**.  
- Advantage: **Reduces variance**, prevents overfitting.  
- Easy to run in **parallel**.  

---

## 3. Boosting  
- Trains models **sequentially** (one after another).  
- Each new model focuses on the errors of the previous one.  
- Uses **low variance, high bias models** as base learners.  
- Example algorithms: **AdaBoost, Gradient Boosting, XGBoost, LightGBM**.  
- Advantage: **Reduces bias**, improves accuracy.  

---

## 4. Stacking  
- Combines predictions of multiple models (**base learners**) using a **meta-model**.  
- Example: Logistic Regression, SVM, Random Forest used as base learners → another model (meta-learner) combines their outputs.  
- Advantage: Learns how to best combine models for better performance.  

---

## 5. Cascading  
- A sequence of models where **output of one model is input to the next**.  
- Often used in **computer vision** (e.g., face detection with Haar cascades).  
- Advantage: Faster processing, as later stages handle fewer candidates.  

---
