# Clustering  

Clustering is an **unsupervised learning technique** used to group similar data points together without predefined labels.  

---

## What Makes a Good Cluster?  
- **Inter-cluster distance** → Distance between different clusters should be large.  
- **Intra-cluster distance** → Distance within a cluster should be small.  

---

## Major Types of Clustering  
1. **K-Means Clustering**  
2. **Hierarchical Clustering**  
   - Agglomerative  
   - Divisive  
3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**  

---

## 1. K-Means Clustering  
- Requires the number of clusters (**k**) to be specified.  
- Works by assigning points to the nearest cluster centroid and updating centroids until convergence.  

### K-Means Code (with Elbow Method)  
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
for i in range(2,5):
    kmeans = KMeans(n_clusters=i, init='k-means++',
                    max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10,6))
plt.plot(range(2,5), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()
```

## 2. Hierarchical Clustering

Agglomerative Hierarchical Clustering

- Starts from smaller clusters and merges them step by step.

- Groups points based on similarity (usually Euclidean distance).

- Can be visualized using dendrograms (tree-like diagrams showing merges/splits).

```python
from sklearn.cluster import AgglomerativeClustering

agg_hc = AgglomerativeClustering(n_clusters=5,
                                 affinity='euclidean',
                                 linkage='ward')

y_hc = agg_hc.fit_predict(X)
```

Divisive Hierarchical Clustering

- Starts from one large cluster and splits into smaller clusters.


## 3. Silhouette Score

- Measures how well clusters are separated and how tightly grouped points are within a cluster.

- Range: -1 to +1 (closer to 1 is better).

```python
from sklearn.metrics import silhouette_score

silhouette_score_value = silhouette_score(X, y_hc)
print("Silhouette Score:", silhouette_score_value)
```

## 4. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

- Groups together points that are closely packed.

- Can handle both dense and sparse regions.

- Identifies outliers as noise (label = -1).

```python
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.3, min_samples=10)
db.fit(X)
labels = db.labels_
```

**Summary:**

- K-Means → Works well on spherical clusters, requires k.

- Hierarchical → Builds a tree-like structure (dendrograms).

- DBSCAN → Handles noise, irregular shapes, and density-based clusters.
