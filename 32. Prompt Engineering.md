# Prompt Engineering

- **Prompt Engineering** is the practice of **designing and refining prompts** to get the best results from AI models, especially **Large Language Models (LLMs)** like GPT.  
- Goal: Make the AI **understand the task correctly** and produce **accurate, relevant, and useful output**.  

---

## Why is Prompt Engineering Important?

- LLMs generate **different outputs based on how the prompt is written**.  
- A well-crafted prompt can:
  - Improve **accuracy and relevance**
  - Reduce **ambiguity**
  - Guide the model for **specific tasks**
  - Save **time and compute resources**

---

## Key Principles of Prompt Engineering

1. **Clarity:** Be specific and clear about the task.  
2. **Context:** Provide relevant background or examples.  
3. **Constraints:** Mention limits like word count, style, or format.  
4. **Step-by-step guidance:** Break complex tasks into smaller steps.  
5. **Examples:** Show input-output pairs to guide the model.  

---

## Types of Prompts

| Prompt Type | Description | Example |
|------------|------------|---------|
| Instructional | Give explicit instructions | "Summarize the following paragraph in 3 sentences." |
| Few-shot | Provide examples in prompt | "Translate English to French. Example: 'Hello' → 'Bonjour'. Translate: 'Good night' → ?" |
| Zero-shot | No examples, just instructions | "Translate 'Good morning' to French." |
| Chain-of-Thought | Guide model step-by-step | "Step 1: Identify key points. Step 2: Summarize." |

---

## Prompt Engineering Techniques

### 1. Zero-shot Prompt
- Model generates output without examples.

```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")
prompt = "Translate the following sentence to Spanish: 'I love data science.'"

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role":"user","content":prompt}]
)

print(response.choices[0].message.content)
```

### 2. Few-shot Prompt

- Provide examples to guide the model.

```python
prompt = """
Translate the following English sentences to French:
1. 'Hello' → 'Bonjour'
2. 'Good night' → 'Bonne nuit'
3. 'I love data science' →
"""

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role":"user","content":prompt}]
)

print(response.choices[0].message.content)
```

### 3. Chain-of-Thought Prompt

- Ask model to reason step-by-step.

```python
prompt = """
Solve the following problem step by step:
If a train travels 60 km in 1 hour, how far will it travel in 4 hours?
"""

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role":"user","content":prompt}]
)

print(response.choices[0].message.content)
```

---

## Best Practices

- Be explicit about the task.

- Provide context for better understanding.

- Limit response length when needed.

- Use structured formats (like JSON) if output needs parsing.

- Experiment with prompt wording to improve results.

---

## Applications of Prompt Engineering

- Text summarization → Generate concise summaries.

- Translation → Convert text between languages.

- Code generation → Write scripts or functions.

- Data analysis → Generate SQL queries or insights.

- Chatbots → Design intelligent conversational agents.

---

## Summary

- Prompt Engineering is critical to getting accurate and useful outputs from LLMs.

- Use clear instructions, context, examples, and step-by-step guidance.

- Experiment with different prompts to maximize model performance.

- Essential for tasks in NLP, AI assistants, code generation, and creative content generation.
